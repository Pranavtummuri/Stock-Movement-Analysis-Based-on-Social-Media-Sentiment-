# Install Required Libraries
# Uncomment the lines below to install libraries if not already installed
# !pip install praw pandas vaderSentiment scikit-learn matplotlib

# Import Libraries
import praw
import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# Section 1: Data Scraping
# Initialize Reddit API
reddit = praw.Reddit(
    client_id='YOUR_CLIENT_ID',
    client_secret='YOUR_CLIENT_SECRET',
    user_agent='YOUR_USER_AGENT'
)

# Scrape data from r/stocks subreddit
print("Scraping data from Reddit...")
subreddit = reddit.subreddit('stocks')
posts = []

for post in subreddit.top(limit=100):  # Adjust limit as needed
    posts.append({
        'title': post.title,
        'selftext': post.selftext,
        'score': post.score,
        'comments': post.num_comments,
        'created_utc': post.created_utc
    })

# Convert scraped data to a DataFrame
df = pd.DataFrame(posts)
print(f"Scraped {len(df)} posts.")
df.head()

# Save raw data
df.to_csv('reddit_stocks_data.csv', index=False)
print("Raw data saved to 'reddit_stocks_data.csv'.")

# Section 2: Data Preprocessing and Sentiment Analysis
# Load scraped data
print("Loading data for sentiment analysis...")
df = pd.read_csv('reddit_stocks_data.csv')

# Initialize VADER Sentiment Analyzer
analyzer = SentimentIntensityAnalyzer()

# Perform sentiment analysis
print("Performing sentiment analysis...")
df['sentiment'] = df['title'].apply(lambda x: analyzer.polarity_scores(x)['compound'])

# Save sentiment data
df.to_csv('reddit_sentiment_data.csv', index=False)
print("Sentiment analysis complete. Data saved to 'reddit_sentiment_data.csv'.")

# Visualize Sentiment Distribution
plt.hist(df['sentiment'], bins=20, color='blue', edgecolor='black')
plt.title('Sentiment Score Distribution')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

# Section 3: Prediction Model
# Prepare features and labels
print("Preparing data for modeling...")
df['movement'] = (df['score'] > df['score'].mean()).astype(int)  # Target variable
X = df[['sentiment']]  # Feature
y = df['movement']      # Label

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression Model
print("Training Logistic Regression model...")
model = LogisticRegression()
model.fit(X_train, y_train)

# Make Predictions
y_pred = model.predict(X_test)

# Evaluate the Model
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Save Model Performance Metrics
with open("model_metrics.txt", "w") as f:
    f.write(f"Accuracy: {accuracy}\n")
    f.write("Classification Report:\n")
    f.write(classification_report(y_test, y_pred))

# Section 4: Save Workflow Outputs
# Save the processed data and model predictions
X_test['actual'] = y_test
X_test['predicted'] = y_pred
X_test.to_csv('model_predictions.csv', index=False)
print("Model predictions saved to 'model_predictions.csv'.")

print("Workflow complete.")
